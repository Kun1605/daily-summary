# Zookeeper

## ZooKeeper 的产生

分布式架构是中心化的设计，就是一个主控机连接多个处理节点。当主控机失效时，整个系统则就无法访问了，所以保证系统的高可用性是非常关键之处，
也就是要保证主控机的高可用性。分布式锁就是一个解决该问题的较好方案，多主控机抢一把锁。

Zookeeper 是雅虎模仿强大的 Google chubby 实现的一套分布式锁管理系统。同时，Zookeeper 分布式服务框架是 Apache Hadoop 的一个子项目，
它是一个针对大型分布式系统的可靠协调系统，它主要是用来**解决分布式应用中经常遇到的一些数据管理问题，可以高可靠的维护元数据**。

提供的功能包括：配置维护、名字服务、分布式同步、组服务等。

ZooKeeper 的设计目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。

## ZooKeeper 的使用

Zookeeper 作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的一致性问题，它能提供基于类似于文件系统的目录节点树方式的数据存储，
但是 Zookeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。

Zookeeper "数据" 是有限制的：

* 从数据大小来看：ZooKeeper 的数据存储在一个叫 ReplicatedDataBase 的数据库中，该数据是一个内存数据库，数据量不会太大，这一点上与 hadoop 的
 HDFS 有了很大的区别，HDFS 的数据主要存储在磁盘上，支持海量数据存储。
* 从数据类型来看：ZooKeeper 的数据在内存中，由于内存空间的限制，所以 ZooKeeper 存储的数据都是我们所关心的数据而且数据量还不能太大，需要根据我们要
实现的功能来选择相应的数据。简单来说，干什么事存什么数据，ZooKeeper 所实现的一切功能，都是由 ZK 节点的性质和该节点所关联的数据实现的。

例如：

1. 集群管理：利用临时节点特性，节点关联的是机器的主机名、IP地址等相关信息，集群单点故障也属于该范畴。
2. 统一命名：主要利用节点的唯一性和目录节点树结构。
3. 配置管理：节点关联的是配置信息。
4. 分布式锁：节点关联的是要竞争的资源。

ZooKeeper 是一个高可用的分布式数据管理与系统协调框架。基于对 Paxos 算法的实现，使该框架保证了分布式环境中数据的强一致性，也正是基于这样的特性，
使得 zookeeper 能够应用于很多场景。

## ZooKeeper 数据结构

![](../images/zk%20数据结构图.png)

1. 每个子目录项如 NameService 都被称作为 znode，这个 znode 是被它所在的路径唯一标识，如 Server1 这个 znode 的标识为 /NameService/Server1；
2. znode 可以有子节点目录，并且每个 znode 可以存储数据，注意 EPHEMERAL 类型的目录节点不能有子节点目录；
3. znode 是有版本的，每个 znode 中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据；
4. znode 可以是临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除，Zookeeper 的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效，znode 也就删除了；
5. znode 的目录名可以自动编号，如 App1 已经存在，再创建的话，将会自动命名为 App2；
6. znode 可以被监控，包括这个目录节点中存储的数据的修改，子节点目录的变化等，一旦变化可以通知设置监控的客户端，这个是 Zookeeper 的核心特性，Zookeeper 的很多功能都是基于这个特性实现的。

## Zookeeper 节点

和文件系统一样，我们能够自由的增加、删除 znode,在 znode 下增加、删除子 znode,唯一不同的在于 znode 是可以存储数据的。

有4种类型的 znode：

1. PERSISTENT--持久化目录节点：客户端与zookeeper断开连接后，该节点依旧存在
2. PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点：客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号
3. EPHEMERAL-临时目录节点：客户端与zookeeper断开连接后，该节点被删除
4. EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点：客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号

## Zookeeper Watch 机制

Zookeeper 客户端在数据节点上设置监视，则当数据节点发生变化时，客户端会收到提醒。

ZooKeeper 中的各种读请求，如 getDate()，getChildren()，和 exists()，都可以选择加"监视点"(watch)。

"监视点"指的是一种一次性的触发器(trigger)，当受监视的数据发生变化时，该触发器会通知客户端。

**监视机制有三个关键点：**
1. "监视点"是一次性的，当触发过一次之后，除非重新设置，新的数据变化不会提醒客户端。
2. "监视点"将数据改变的通知客户端。如果数据改变是客户端A引起的，不能保证"监视点"通知事件会在引发数据修改的函数返回前到达客户端A。
3. 对于"监视点"，ZooKeeper 有如下保证：客户端一定是在接收到"监视"事件（watch event）之后才接收到数据的改变信息。

"监视点"保留在 ZooKeeper 服务器上，则当客户端连接到新的 ZooKeeper 服务器上时，所有需要被触发的相关"监视点"都会被触发。当客户端断线后重连，
与它的相关的"监视点"都会自动重新注册，这对客户端来说是透明的。在以下情况，"监视点"会被错过：客户端 B 设置了关于节点A存在性的"监视点"，但 B 断线了，
在 B 断线过程中节点 A 被创建又被删除。此时，B 再连线后不知道 A 节点曾经被创建过。

**ZooKeeper 的"监视"机制保证以下几点：**
1. "监视"事件的触发顺序和事件的分发顺序一致
2. 客户端将先接收到"监视"事件，然后才收到新的数据
3. "监视"事件触发的顺序与 ZooKeeper 服务器上数据变化的顺序一致

**关于 ZooKeeper"监视"机制的注意点：**
1. "监视点"是一次性的
2. 由于"监视点"是一次性的，而且，从接收到"监视"事件到设置新"监视点"是有延时的，所以客户端可能监控不到数据的所有变化。
3. 一个监控对象，只会被相关的通知触发一次。如果一个客户端设置了关于某个数据点 exists 和 getData 的监控，则当该数据被删除的时候，只会触发"文件被删除"的
通知。
4. 当客户端断开与服务器的连接时，客户端不再能收到"监视"事件，直到重新获得连接。所以关于 Session 的信息将被发送给所有 ZooKeeper 服务器。由于当连接断开时收不到"监视"，所以在这种情况下，模块行为需要容错方面的设计。

## Zookeeper 权限管理机制

### 权限管理 ACL(Access Control List)　

ZooKeeper 的权限管理亦即 ACL 控制功能，使用 ACL 来对 Znode 进行访问控制。ACL 的实现和 Unix 文件访问许可非常相似：
它使用许可位来对一个节点的不同操作进行允许或禁止的权限控制。

ZooKeeper 的权限管理通过 Server、Client 两端协调完成。

#### Server端

一个 ZooKeeper 的节点存储两部分内容：数据和状态，状态中包含ACL 信息。创建一个 znode 会产生一个 ACL 列表，列表中每个 ACL 包括：
1. 权限 perms
2. 验证模式 scheme
3. 具体内容 expression：Ids

**ZooKeeper 提供了如下几种验证模式：**
1. Digest： Client 端由用户名和密码验证，譬如 user:pwd
3. Ip：Client 端由 IP 地址验证，譬如 172.2.0.0/24
4. World ：固定用户为 anyone，为所有 Client 端开放权限

当会话建立的时候，客户端将会进行自我验证。例如，当 scheme="digest" 时， Ids 为用户名密码， 即 "root ：J0sTy9BCUKubtK1y8pkbL7qoxSw"。

**权限许可集合如下：**
1. Create 允许对子节点 Create 操作
2. Read 允许对本节点 GetChildren 和 GetData 操作
3. Write 允许对本节点 SetData 操作
4. Delete 允许对子节点 Delete 操作
5. Admin 允许对本节点 setAcl 操作

另外，ZooKeeper Java API 支持三种标准的用户权限，它们分别为：
1. ZOO_PEN_ACL_UNSAFE：对于所有的ACL来说都是完全开放的，任何应用程序可以在节点上执行任何操作，比如创建、列出并删除子节点。
2. ZOO_READ_ACL_UNSAFE：对于任意的应用程序来说，仅仅具有读权限。
3. ZOO_CREATOR_ALL_ACL：授予节点创建者所有权限。需要注意的是，设置此权限之前，创建者必须已经通了服务器的认证。

Znode ACL 权限用一个 int 型数字p erms 表示， perms 的 5 个二进制位分别表示 setacl、delete、create、write、read。

比如adcwr=0x1f(11111)，----r=0x1(00001)，a-c-r=0x15(10101)。

**注意的是，exists 操作和 getAcl 操作并不受 ACL许可控制，因此任何客户端可以查询节点的状态和节点的 ACL。**

#### 客户端

Client 通过调用 addAuthInfo() 函数设置当前会话的 Author 信息（针对 Digest 验证模式）。

Server 收到 Client 发送的操作请求（除 exists、getAcl 之外），需要进行 ACL 验证：
对该请求携带的 Author 明文信息加密，并与目标节点的 ACL 信息进行比较，如果匹配则具有相应的权限，否则请求被 Server 拒绝。

一次 Client 对 Znode 进行操作的验证 ACL 的方式为，遍历 znode 的所有 ACL：
1. 对于每一个 ACL，首先操作类型与权限（perms）匹配
2. 只有匹配权限成功才进行 session 的 auth 信息与 ACL 的用户名、密码匹配
3. 如果两次匹配都成功，则允许操作；否则，返回权限不够 error（rc=-102）

## ZooKeeper 异常

在 Java API 中的每一个 ZooKeeper 操作都在其 throws 子句中声明了两种类型的异常，分别是 InterruptedException 和 KeeperException。

（一）InterruptedException 异常

如果操作被中断，则会有一个 InterruptedException 异常被抛出。在 Java 语言中有一个取消阻塞方法的标准机制，即针对存在阻塞方法的线程调用 interrupt()。
一个成功的取消操作将产生一个 InterruptedException 异常。

ZooKeeper 也遵循这一机制，因此你可以使用这种方法来取消一个 ZooKeeper 操作。使用了 ZooKeeper 的类或库通常会传播 InterruptedException 异常，使客户端能够取消它们的操作。
InterruptedException 异常并不意味着有故障，而是表明相应的操作已经被取消，所以在配置服务的示例中，可以通过传播异常来中止应用程序的运行。

（二）KeeperException异常

(1) 如果 ZooKeeper 服务器发出一个错误信号或与服务器存在通信问题，抛出的则是 KeeperException 异常。

**针对不同的错误情况，KeeperException 异常存在不同的子类。**

例如:　KeeperException.NoNodeException 是 KeeperException 的一个子类，如果你试图针对一个不存在的 znode 执行操作，抛出的则是该异常。

**每一个 KeeperException 异常的子类都对应一个关于错误类型信息的代码。**

例如:　KeeperException.NoNodeException 异常的代码是 KeeperException.Code.NONODE

**有两种方法被用来处理 KeeperException 异常。**

1. 捕捉 KeeperException 异常，并且通过检测它的代码来决定采取何种补救措施；
2. 另一种是捕捉等价的 KeeperException 子类，并且在每段捕捉代码中执行相应的操作。

(3) KeeperException 异常分为三大类

* 状态异常 

当一个操作因不能被应用于 znode 树而导致失败时，就会出现状态异常。状态异常产生的原因通常是在同一时间有另外一个进程正在修改znode。
例如，如果一个 znode 先被另外一个进程更新了，根据版本号执行 setData 操作的进程就会失败，并收到一个 KeeperException.BadVersionException 异常，
这是因为版本号不匹配。程序员通常都知道这种冲突总是存在的，也都会编写代码来进行处理。

一些状态异常会指出程序中的错误，例如 KeeperException.NoChildrenForEphemeralsException 异常，试图在短暂 znode 下创建子节点时就会抛出该异常。

* 可恢复异常

可恢复的异常是指那些应用程序能够在同一个 ZooKeeper 会话中恢复的异常。一个可恢复的异常是通过 KeeperException.ConnectionLossException 来表示的，
它意味着已经丢失了与 ZooKeeper 的连接。ZooKeeper 会尝试重新连接，并且在大多数情况下重新连接会成功，并确保会话是完整的。

但是 ZooKeeper 不能判断与 KeeperException.ConnectionLossException 异常相关的操作是否成功执行。这种情况就是部分失败的一个例子。
这时程序员有责任来解决这种不确定性，并且根据应用的情况来采取适当的操作。在这一点上，就需要对“幂等”(idempotent)操作和“非幂等”(Nonidempotent)操作进行区分。
幂等操作是指那些一次或多次执行都会产生相同结果的操作，例如读请求或无条件执行的 setData 操作。对于幂等操作，只需要简单地进行重试即可。对于非幂等操作，就不能盲目地进行重试，
因为它们多次执行的结果与一次执行是完全不同的。程序可以通过在 znode 的路径和它的数据中编码信息来检测是否非幂等操怍的更新已经完成。

* 不可恢复的异常 

在某些情况下，ZooKeeper 会话会失效——也许因为超时或因为会话被关闭，两种情况下都会收到 KeeperException.SessionExpiredException 异常，
或因为身份验证失败，KeeperException.AuthFailedException 异常。无论上述哪种情况，所有与会话相关联的短暂 znode 都将丢失，
因此应用程序需要在重新连接到 ZooKeeper 之前重建它的状态。

## Session 机制

每个 ZooKeeper 客户端的配置中都包括集合体中服务器的列表。在启动时，客户端会尝试连接到列表中的一台服务器。如果连接失败，它会尝试连接另一台服务器，
以此类推，直到成功与一台服务器建立连接或因为所有 ZooKeeper 服务器都不可用而失败。

![](../images/zk%20体系结构.png)

一旦客户端与一台 ZooKeeper 服务器建立连接，这台服务器就会为该客户端创建一个新的会话。每个会话都会有一个超时的时间设置，这个设置由创建会话的应用来设定。
如果服务器在超时时间段内没有收到任何请求，则相应的会话会过期。一旦一个会话已经过期，就无法重新打开，并且任何与该会话相关联的短暂 znode 都会丢失。
会话通常长期存在，而且会话过期是一种比较罕见的事件，但对应用来说，如何处理会话过期仍是非常重要的。

只要一个会话空闲超过一定时间，都可以通过客户端发送 ping 请求（也称为心跳）保持会话不过期。ping 请求由 ZooKeeper 的客户端库自动发送，因此在我们的代码
中不需要考虑如何维护会话。这个时间长度的设置应当足够低，以便能档检测出服务器故障（由读超时体现），并且能够在会话超时的时间段内重新莲接到另外一台服务器。

### 故障切换

ZooKeeper 客户端可以自动地进行故障切换，切换至另一台 ZooKeeper 服务器。并且关键的一点是，在另一台服务器接替故障服务器之后，所有的会话和相关的短暂 Znode 仍然是有效的。
在故障切换过程中，应用程序将收到断开连接和连接至服务的通知。当客户端断开连接时，观察通知将无法发送；但是当客户端成功恢复连接后，这些延迟的通知会被发送。
当然，在客户端重新连接至另一台服务器的过程中，如果应用程序试图执行一个操作，这个操作将会失败。这充分体现了在真实的 ZooKeeper 应用中处理连接丢失异常的重要性。

## ZooKeeper 实例状态

#### ZooKeeper 状态

ZooKeeper 对象在其生命周期中会经历几种不同的状态。可以在任何时刻通过 getState() 方法来查询对象的状态：

public States getState()

States 被定义成代表 ZooKeeper 对象不同状态的枚举类型值（不管是什么枚举值，一个 ZooKeeper 的实例在一个时刻只能处于一种状态）。
在试图与 ZooKeeper 服务建立连接的过程中，一个新建的 ZooKeeper 实例处于 CONNECTING 状态。一旦建立连接，它就会进入 CONNECTED 状态。 

![](../images/zk%20状态转换.png)

通过注册观察对象，使用了 ZooKeeper 对象的客户端可以收到状态转换通知。在进入 CONNECTED 状态时，观察对象会收到一个 WatchedEvent 通知，
其中 KeeperState 的值是 SyncConnected。

### Watch 与 ZooKeeper 状态

ZooKeeper的观察对象肩负着双重责任：
1. 可以用来获得 ZooKeeper 状态变化的相关通知；
2. 可以用来获得 Znode 变化的相关通知。

监视 ZooKeeper 状态变化：可以使用 ZooKeeper 对象默认构造函数的观察。

监视 Znode 变化：可以使用一个专用的观察对象，将其传递给适当的读操作。也可以通过读操作中的布尔标识来设定是否共享使用默认的观察。

ZooKeeper 实例可能失去或重新连接 ZooKeeper 服务，在 CONNECTED 和 CONNECTING 状态中切换。如果连接断开，watcher 得到一个 Disconnected 事件。
要注意的是，这些状态的迁移是由 ZooKeeper 实例自己发起的，如果连接断开他将自动尝试自动连接。

如果任何一个 close() 方法被调用，或是会话由 Expired 类型的 KeepState 提示过期时，ZooKeeper 可能会转变成第三种状态 CLOSED。一旦处于 CLOSED 状态，
ZooKeeper 对象将不再是活动的了(可以使用 states 的 isActive() 方法进行测试)，而且不能被重用。客户端必须建立一个新的 ZooKeeper 实例才能重新连接到 ZooKeeper 服务。

## ZooKeeper 应用场景

### 数据发布与订阅

发布与订阅即所谓的配置管理，顾名思义就是将数据发布到 ZK 节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，地址列表等就非常适合使用。

集中式的配置管理在应用集群中是非常常见的，一般商业公司内部都会实现一套集中的配置管理中心，应对不同的应用集群对于共享各自配置的需求，并且在配置变更时能够通知到集群中的每一个机器。

例如：同一个应用系统需要多台 PC Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 PC Server，这样非常麻烦而且容易出错。
将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中。
ZooKeeper 配置管理服务如下图所示：

![](../images/zk%20配置管理结构图.png)

Zookeeper 很容易实现这种集中式的配置管理，比如将所需要的配置信息放到 /Configuration 节点上，集群中所有机器一启动就会通过 Client 对 /Configuration 这个节点进行监控【zk.exist("/Configuration″,true)】，
并且实现 Watcher 回调方法 process()，那么在 zookeeper 上 /Configuration 节点下数据发生变化的时候，每个机器都会收到通知，Watcher 回调方法将会被执行，那么应用再取下数据即可【zk.getData("/Configuration″,false,null)】。

### 统一命名服务（Name Service）

分布式应用中，通常需要有一套完整的命名规则，既能够产生唯一的名称又便于人识别和记住，通常情况下用树形的名称结构是一个理想的选择，树形的名称结构是一个有层次的目录结构，既对人友好又不会重复。
说到这里你可能想到了 JNDI，没错 Zookeeper 的 Name Service 与 JNDI 能够完成的功能是差不多的，它们都是将有层次的目录结构关联到一定资源上，但是 Zookeeper 的 Name Service 
更加是广泛意义上的关联，也许你并不需要将名称关联到特定资源上，你可能只需要一个不会重复名称，就像数据库中产生一个唯一的数字主键一样。

在分布式系统中，通过使用命名服务，客户端应用能够根据指定的名字来获取资源服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，进程对象等等，这些可以统称他们为名字（Name）。
其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用 ZK 提供的创建节点的 API，能够很容易创建一个全局唯一的 path，这个 path 就可以作为一个名称。

Name Service 已经是 Zookeeper 内置的功能，只要调用 Zookeeper 的 API 就能实现。如调用 create 接口就可以很容易创建一个目录节点。

例如：阿里开源的分布式服务框架 Dubbo 中使用 ZooKeeper 来作为其命名服务，维护全局的服务地址列表。在 Dubbo 实现中： 服务提供者在启动的时候，向 ZK 上的指定节点/dubbo/${serviceName}/providers 目录下写入自己的 URL 地址，这个操作就完成了服务的发布。 
服务消费者启动的时候，订阅 /dubbo/${serviceName}/providers 目录下的提供者 URL 地址， 并向 /dubbo/${serviceName}/consumers 目录下写入自己的 URL 地址。 

注意，所有向 ZK 上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。 另外，Dubbo 还有针对服务粒度的监控，方法是订阅 /dubbo/${serviceName} 目录下所有提供者和消费者的信息。

### 分布通知/协调（Distribution of notification/coordination）

ZooKeeper 中特有 watcher 注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对 ZK 上同一个 znode 进行注册，监听 znode 的变化（包括 znode 本身内容及子节点的），
其中一个系统 update 了 znode，那么另一个系统能够收到通知，并作出相应处理。

例如：
1. 另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过 ZK 上某个节点关联，大大减少系统耦合。
2. 另一种系统调度模式：某系统由控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了 ZK 上某些节点的状态，而ZK就把这些变化通知给他们注册 Watcher 的客户端，即推送系统，于是，作出相应的推送任务。
3. 另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到 ZK 来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。

使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合。

### 分布式锁（Distribute Lock）

分布式锁，这个主要得益于 ZooKeeper 提供的数据的强一致性，即用户只要完全相信每时每刻，zk 集群中任意节点（一个zk server）上的相同 znode 的数据是一定是相同的。锁服务可以分为两类，一个是保持独占，另一个是控制时序。

保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把 ZK 上的一个 znode 看作是一把锁，通过 create znode 的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。

控制时序，就是所有试图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经预先存在，客户端在它下面创建临时有序节点。Zk 的父节点（/distribute_lock）维持一份 sequence，保证子节点创建的时序性，从而也形成了每个客户端的全局时序。

例如：共享锁在同一个进程中很容易实现，但是在跨进程或者在不同 Server 之间就不好实现了。
Zookeeper 却很容易实现这个功能，实现方式也是需要获得锁的 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，然后调用 getChildren 方法获取当前的目录节点列表中最小的目录节点是不是就是自己创建的目录节点，如果正是自己创建的，那么它就获得了这个锁，如果不是那么它就调用 exists(String path, boolean watch) 方法并监控 Zookeeper 上目录节点列表的变化，
一直到自己创建的节点是列表中最小编号的目录节点，从而获得锁，释放锁很简单，只要删除前面它自己所创建的目录节点就行了。

### 集群管理（Cluster Management）

**集群机器监控：**

这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。

过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报"我还活着"。 这种做法可行，但是存在两个比较明显的问题：
1. 集群中机器有变动的时候，牵连修改的东西比较多。
2. 有一定的延时。

利用 ZooKeeper 中两个特性，就可以实施另一种集群机器存活性监控系统：
1. 客户端在节点 x 上注册一个 Watcher，那么如果 x 的子节点变化了，会通知该客户端。
2. 创建 EPHEMERAL 类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。

例如：应用集群中，常常需要让每一个机器知道集群中或依赖的其他某一个集群中哪些机器是活着的，并且在集群机器因为宕机，网络断链等原因能够不在人工介入的情况下迅速通知到每一个机器，
Zookeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个"总管"知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，
从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让"总管"知道，这就是 ZooKeeper 的集群监控功能。

![](../images/zk%20集群管理结构图.png)

在 zookeeper 服务器端有一个 znode 叫 /Configuration，那么集群中每一个机器启动的时候都去这个节点下创建一个 EPHEMERAL 类型的节点，比如 server1 创建 /Configuration/Server1，
server2 创建 /Configuration/Server2，然后 Server1 和 Server2 都 watch /Configuration 这个父节点，那么也就是这个父节点下数据或者子节点变化都会通知对该节点进行 watch 的客户端。
因为 EPHEMERAL 类型节点有一个很重要的特性，就是客户端和服务器端连接断掉或者 session 过期就会使节点消失，那么在某一个机器挂掉或者断链的时候，其对应的节点就会消 失，然后集群中所有对
 /Configuration 进行 watch 的客户端都会收到通知，然后取得最新列表即可。

**Master选举：**

Master 选举则是 zookeeper 中最为经典的使用场景了，在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑，例如一些耗时的计算，网络 I/O 处，
往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个 master 选举便是这种场景下的碰到的主要问题。

利用 ZooKeeper 中两个特性，就可以实施另一种集群中Master选举：
1. 利用 **ZooKeeper 的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性**，即：同时有多个客户端请求创建 /Master 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选举了。
2. 另外，这种场景演化一下，就是动态 Master 选举。这就要用到 EPHEMERAL_SEQUENTIAL 类型节点的特性了，这样每个节点会自动被编号。允许所有请求都能够创建成功，但是得有个创建顺序，每次选取序列号最小的那个机器作为 Master 。

例如：每台 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，这样可以给每台 Server 编号，我们可以选择当前是最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，
所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。
