# Zookeeper

## ZooKeeper 的产生

分布式架构是中心化的设计，就是一个主控机连接多个处理节点。当主控机失效时，整个系统则就无法访问了，所以保证系统的高可用性是非常关键之处，
也就是要保证主控机的高可用性。分布式锁就是一个解决该问题的较好方案，多主控机抢一把锁。

Zookeeper 是雅虎模仿强大的 Google chubby 实现的一套分布式锁管理系统。同时，Zookeeper 分布式服务框架是 Apache Hadoop 的一个子项目，
它是一个针对大型分布式系统的可靠协调系统，它主要是用来**解决分布式应用中经常遇到的一些数据管理问题，可以高可靠的维护元数据**。

提供的功能包括：配置维护、名字服务、分布式同步、组服务等。

ZooKeeper 的设计目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。

## ZooKeeper 的使用

Zookeeper 作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的一致性问题，它能提供基于类似于文件系统的目录节点树方式的数据存储，
但是 Zookeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控存储的数据的状态变化。通过监控这些数据状态的变化，从而可以达到基于数据的集群管理。

Zookeeper "数据" 是有限制的：

* 从数据大小来看：ZooKeeper 的数据存储在一个叫 ReplicatedDataBase 的数据库中，该数据是一个内存数据库，数据量不会太大，这一点上与 hadoop 的
 HDFS 有了很大的区别，HDFS 的数据主要存储在磁盘上，支持海量数据存储。
* 从数据类型来看：ZooKeeper 的数据在内存中，由于内存空间的限制，所以 ZooKeeper 存储的数据都是我们所关心的数据而且数据量还不能太大，需要根据我们要
实现的功能来选择相应的数据。简单来说，干什么事存什么数据，ZooKeeper 所实现的一切功能，都是由 ZK 节点的性质和该节点所关联的数据实现的。

例如：

1. 集群管理：利用临时节点特性，节点关联的是机器的主机名、IP地址等相关信息，集群单点故障也属于该范畴。
2. 统一命名：主要利用节点的唯一性和目录节点树结构。
3. 配置管理：节点关联的是配置信息。
4. 分布式锁：节点关联的是要竞争的资源。

ZooKeeper 是一个高可用的分布式数据管理与系统协调框架。基于对 Paxos 算法的实现，使该框架保证了分布式环境中数据的强一致性，也正是基于这样的特性，
使得 zookeeper 能够应用于很多场景。

## ZooKeeper 数据结构

![](../images/zk%20数据结构图.png)

1. 每个子目录项如 NameService 都被称作为 znode，这个 znode 是被它所在的路径唯一标识，如 Server1 这个 znode 的标识为 /NameService/Server1；
2. znode 可以有子节点目录，并且每个 znode 可以存储数据，注意 EPHEMERAL 类型的目录节点不能有子节点目录；
3. znode 是有版本的，每个 znode 中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据；
4. znode 可以是临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除，Zookeeper 的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效，znode 也就删除了；
5. znode 的目录名可以自动编号，如 App1 已经存在，再创建的话，将会自动命名为 App2；
6. znode 可以被监控，包括这个目录节点中存储的数据的修改，子节点目录的变化等，一旦变化可以通知设置监控的客户端，这个是 Zookeeper 的核心特性，Zookeeper 的很多功能都是基于这个特性实现的。

## Zookeeper 节点

和文件系统一样，我们能够自由的增加、删除 znode,在 znode 下增加、删除子 znode,唯一不同的在于 znode 是可以存储数据的。

有4种类型的 znode：

1. PERSISTENT--持久化目录节点：客户端与zookeeper断开连接后，该节点依旧存在
2. PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点：客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号
3. EPHEMERAL-临时目录节点：客户端与zookeeper断开连接后，该节点被删除
4. EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点：客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号

## ZooKeeper 异常

在 Java API 中的每一个 ZooKeeper 操作都在其 throws 子句中声明了两种类型的异常，分别是 InterruptedException 和 KeeperException。

（一）InterruptedException 异常

如果操作被中断，则会有一个 InterruptedException 异常被抛出。在 Java 语言中有一个取消阻塞方法的标准机制，即针对存在阻塞方法的线程调用 interrupt()。
一个成功的取消操作将产生一个 InterruptedException 异常。

ZooKeeper 也遵循这一机制，因此你可以使用这种方法来取消一个 ZooKeeper 操作。使用了 ZooKeeper 的类或库通常会传播 InterruptedException 异常，使客户端能够取消它们的操作。
InterruptedException 异常并不意味着有故障，而是表明相应的操作已经被取消，所以在配置服务的示例中，可以通过传播异常来中止应用程序的运行。

（二）KeeperException异常

(1) 如果 ZooKeeper 服务器发出一个错误信号或与服务器存在通信问题，抛出的则是 KeeperException 异常。

①针对不同的错误情况，KeeperException 异常存在不同的子类。

例如:　KeeperException.NoNodeException 是 KeeperException 的一个子类，如果你试图针对一个不存在的 znode 执行操作，抛出的则是该异常。

②每一个 KeeperException 异常的子类都对应一个关于错误类型信息的代码。

例如:　KeeperException.NoNodeException 异常的代码是 KeeperException.Code.NONODE

(2) 有两种方法被用来处理 KeeperException 异常：

①捕捉 KeeperException 异常，并且通过检测它的代码来决定采取何种补救措施；

②另一种是捕捉等价的 KeeperException 子类，并且在每段捕捉代码中执行相应的操作。

(3) KeeperException 异常分为三大类

① 状态异常 

当一个操作因不能被应用于 znode 树而导致失败时，就会出现状态异常。状态异常产生的原因通常是在同一时间有另外一个进程正在修改znode。
例如，如果一个 znode 先被另外一个进程更新了，根据版本号执行 setData 操作的进程就会失败，并收到一个 KeeperException.BadVersionException 异常，
这是因为版本号不匹配。程序员通常都知道这种冲突总是存在的，也都会编写代码来进行处理。

一些状态异常会指出程序中的错误，例如 KeeperException.NoChildrenForEphemeralsException 异常，试图在短暂 znode 下创建子节点时就会抛出该异常。

② 可恢复异常

可恢复的异常是指那些应用程序能够在同一个 ZooKeeper 会话中恢复的异常。一个可恢复的异常是通过 KeeperException.ConnectionLossException 来表示的，
它意味着已经丢失了与 ZooKeeper 的连接。ZooKeeper 会尝试重新连接，并且在大多数情况下重新连接会成功，并确保会话是完整的。

但是 ZooKeeper 不能判断与 KeeperException.ConnectionLossException 异常相关的操作是否成功执行。这种情况就是部分失败的一个例子。
这时程序员有责任来解决这种不确定性，并且根据应用的情况来采取适当的操作。在这一点上，就需要对“幂等”(idempotent)操作和“非幂等”(Nonidempotent)操作进行区分。
幂等操作是指那些一次或多次执行都会产生相同结果的操作，例如读请求或无条件执行的 setData 操作。对于幂等操作，只需要简单地进行重试即可。对于非幂等操作，就不能盲目地进行重试，
因为它们多次执行的结果与一次执行是完全不同的。程序可以通过在 znode 的路径和它的数据中编码信息来检测是否非幂等操怍的更新已经完成。

③不可恢复的异常 

在某些情况下，ZooKeeper 会话会失效——也许因为超时或因为会话被关闭，两种情况下都会收到 KeeperException.SessionExpiredException 异常，
或因为身份验证失败，KeeperException.AuthFailedException 异常。无论上述哪种情况，所有与会话相关联的短暂 znode 都将丢失，
因此应用程序需要在重新连接到 ZooKeeper 之前重建它的状态。

## ZooKeeper 应用场景

### 数据发布与订阅

发布与订阅即所谓的配置管理，顾名思义就是将数据发布到 ZK 节点上，供订阅者动态获取数据，实现配置信息的集中式管理和动态更新。例如全局的配置信息，地址列表等就非常适合使用。

集中式的配置管理在应用集群中是非常常见的，一般商业公司内部都会实现一套集中的配置管理中心，应对不同的应用集群对于共享各自配置的需求，并且在配置变更时能够通知到集群中的每一个机器。

例如：同一个应用系统需要多台 PC Server 运行，但是它们运行的应用系统的某些配置项是相同的，如果要修改这些相同的配置项，那么就必须同时修改每台运行这个应用系统的 PC Server，这样非常麻烦而且容易出错。
将配置信息保存在 Zookeeper 的某个目录节点中，然后将所有需要修改的应用机器监控配置信息的状态，一旦配置信息发生变化，每台应用机器就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中。
ZooKeeper 配置管理服务如下图所示：

![](../images/zk%20配置管理结构图.png)

Zookeeper 很容易实现这种集中式的配置管理，比如将所需要的配置信息放到 /Configuration 节点上，集群中所有机器一启动就会通过 Client 对 /Configuration 这个节点进行监控【zk.exist("/Configuration″,true)】，
并且实现 Watcher 回调方法 process()，那么在 zookeeper 上 /Configuration 节点下数据发生变化的时候，每个机器都会收到通知，Watcher 回调方法将会被执行，那么应用再取下数据即可【zk.getData("/Configuration″,false,null)】。

### 统一命名服务（Name Service）

分布式应用中，通常需要有一套完整的命名规则，既能够产生唯一的名称又便于人识别和记住，通常情况下用树形的名称结构是一个理想的选择，树形的名称结构是一个有层次的目录结构，既对人友好又不会重复。
说到这里你可能想到了 JNDI，没错 Zookeeper 的 Name Service 与 JNDI 能够完成的功能是差不多的，它们都是将有层次的目录结构关联到一定资源上，但是 Zookeeper 的 Name Service 
更加是广泛意义上的关联，也许你并不需要将名称关联到特定资源上，你可能只需要一个不会重复名称，就像数据库中产生一个唯一的数字主键一样。

在分布式系统中，通过使用命名服务，客户端应用能够根据指定的名字来获取资源服务的地址，提供者等信息。被命名的实体通常可以是集群中的机器，提供的服务地址，进程对象等等，这些可以统称他们为名字（Name）。
其中较为常见的就是一些分布式服务框架中的服务地址列表。通过调用 ZK 提供的创建节点的 API，能够很容易创建一个全局唯一的 path，这个 path 就可以作为一个名称。

Name Service 已经是 Zookeeper 内置的功能，只要调用 Zookeeper 的 API 就能实现。如调用 create 接口就可以很容易创建一个目录节点。

例如：阿里开源的分布式服务框架 Dubbo 中使用 ZooKeeper 来作为其命名服务，维护全局的服务地址列表。在 Dubbo 实现中： 服务提供者在启动的时候，向 ZK 上的指定节点/dubbo/${serviceName}/providers 目录下写入自己的 URL 地址，这个操作就完成了服务的发布。 
服务消费者启动的时候，订阅 /dubbo/${serviceName}/providers 目录下的提供者 URL 地址， 并向 /dubbo/${serviceName}/consumers 目录下写入自己的 URL 地址。 

注意，所有向 ZK 上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能够自动感应资源的变化。 另外，Dubbo 还有针对服务粒度的监控，方法是订阅 /dubbo/${serviceName} 目录下所有提供者和消费者的信息。

### 分布通知/协调（Distribution of notification/coordination）

ZooKeeper 中特有 watcher 注册与异步通知机制，能够很好的实现分布式环境下不同系统之间的通知与协调，实现对数据变更的实时处理。使用方法通常是不同系统都对 ZK 上同一个 znode 进行注册，监听 znode 的变化（包括 znode 本身内容及子节点的），
其中一个系统 update 了 znode，那么另一个系统能够收到通知，并作出相应处理。

例如：
1. 另一种心跳检测机制：检测系统和被检测系统之间并不直接关联起来，而是通过 ZK 上某个节点关联，大大减少系统耦合。
2. 另一种系统调度模式：某系统由控制台和推送系统两部分组成，控制台的职责是控制推送系统进行相应的推送工作。管理人员在控制台作的一些操作，实际上是修改了 ZK 上某些节点的状态，而ZK就把这些变化通知给他们注册 Watcher 的客户端，即推送系统，于是，作出相应的推送任务。
3. 另一种工作汇报模式：一些类似于任务分发系统，子任务启动后，到 ZK 来注册一个临时节点，并且定时将自己的进度进行汇报（将进度写回这个临时节点），这样任务管理者就能够实时知道任务进度。

使用zookeeper来进行分布式通知和协调能够大大降低系统之间的耦合。

### 分布式锁（Distribute Lock）

分布式锁，这个主要得益于 ZooKeeper 提供的数据的强一致性，即用户只要完全相信每时每刻，zk 集群中任意节点（一个zk server）上的相同 znode 的数据是一定是相同的。锁服务可以分为两类，一个是保持独占，另一个是控制时序。

保持独占，就是所有试图来获取这个锁的客户端，最终只有一个可以成功获得这把锁。通常的做法是把 ZK 上的一个 znode 看作是一把锁，通过 create znode 的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。

控制时序，就是所有试图来获取这个锁的客户端，最终都是会被安排执行，只是有个全局时序了。做法和上面基本类似，只是这里 /distribute_lock 已经预先存在，客户端在它下面创建临时有序节点。Zk 的父节点（/distribute_lock）维持一份 sequence，保证子节点创建的时序性，从而也形成了每个客户端的全局时序。

例如：共享锁在同一个进程中很容易实现，但是在跨进程或者在不同 Server 之间就不好实现了。
Zookeeper 却很容易实现这个功能，实现方式也是需要获得锁的 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，然后调用 getChildren 方法获取当前的目录节点列表中最小的目录节点是不是就是自己创建的目录节点，如果正是自己创建的，那么它就获得了这个锁，如果不是那么它就调用 exists(String path, boolean watch) 方法并监控 Zookeeper 上目录节点列表的变化，
一直到自己创建的节点是列表中最小编号的目录节点，从而获得锁，释放锁很简单，只要删除前面它自己所创建的目录节点就行了。

### 集群管理（Cluster Management）

**集群机器监控：**

这通常用于那种对集群中机器状态，机器在线率有较高要求的场景，能够快速对集群中机器变化作出响应。这样的场景中，往往有一个监控系统，实时检测集群机器是否存活。

过去的做法通常是：监控系统通过某种手段（比如ping）定时检测每个机器，或者每个机器自己定时向监控系统汇报"我还活着"。 这种做法可行，但是存在两个比较明显的问题：
1. 集群中机器有变动的时候，牵连修改的东西比较多。
2. 有一定的延时。

利用 ZooKeeper 中两个特性，就可以实施另一种集群机器存活性监控系统：
1. 客户端在节点 x 上注册一个 Watcher，那么如果 x 的子节点变化了，会通知该客户端。
2. 创建 EPHEMERAL 类型的节点，一旦客户端和服务器的会话结束或过期，那么该节点就会消失。

例如：应用集群中，常常需要让每一个机器知道集群中或依赖的其他某一个集群中哪些机器是活着的，并且在集群机器因为宕机，网络断链等原因能够不在人工介入的情况下迅速通知到每一个机器，
Zookeeper 能够很容易的实现集群管理的功能，如有多台 Server 组成一个服务集群，那么必须要一个"总管"知道当前集群中每台机器的服务状态，一旦有机器不能提供服务，集群中其它集群必须知道，
从而做出调整重新分配服务策略。同样当增加集群的服务能力时，就会增加一台或多台 Server，同样也必须让"总管"知道，这就是 ZooKeeper 的集群监控功能。

![](../images/zk%20集群管理结构图.png)

在 zookeeper 服务器端有一个 znode 叫 /Configuration，那么集群中每一个机器启动的时候都去这个节点下创建一个 EPHEMERAL 类型的节点，比如 server1 创建 /Configuration/Server1，
server2 创建 /Configuration/Server2，然后 Server1 和 Server2 都 watch /Configuration 这个父节点，那么也就是这个父节点下数据或者子节点变化都会通知对该节点进行 watch 的客户端。
因为 EPHEMERAL 类型节点有一个很重要的特性，就是客户端和服务器端连接断掉或者 session 过期就会使节点消失，那么在某一个机器挂掉或者断链的时候，其对应的节点就会消 失，然后集群中所有对
 /Configuration 进行 watch 的客户端都会收到通知，然后取得最新列表即可。

**Master选举：**

Master 选举则是 zookeeper 中最为经典的使用场景了，在分布式环境中，相同的业务应用分布在不同的机器上，有些业务逻辑，例如一些耗时的计算，网络 I/O 处，
往往只需要让整个集群中的某一台机器进行执行，其余机器可以共享这个结果，这样可以大大减少重复劳动，提高性能，于是这个 master 选举便是这种场景下的碰到的主要问题。

利用 ZooKeeper 中两个特性，就可以实施另一种集群中Master选举：
1. 利用 **ZooKeeper 的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性**，即：同时有多个客户端请求创建 /Master 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选举了。
2. 另外，这种场景演化一下，就是动态 Master 选举。这就要用到 EPHEMERAL_SEQUENTIAL 类型节点的特性了，这样每个节点会自动被编号。允许所有请求都能够创建成功，但是得有个创建顺序，每次选取序列号最小的那个机器作为 Master 。

例如：每台 Server 创建一个 EPHEMERAL_SEQUENTIAL 目录节点，这样可以给每台 Server 编号，我们可以选择当前是最小编号的 Server 为 Master，假如这个最小编号的 Server 死去，由于是 EPHEMERAL 节点，死去的 Server 对应的节点也被删除，
所以当前的节点列表中又出现一个最小编号的节点，我们就选择这个节点为当前 Master。这样就实现了动态选择 Master，避免了传统意义上单 Master 容易出现单点故障的问题。
