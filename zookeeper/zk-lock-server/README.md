## ZooKeeper 实现共享锁

### 分布式锁概述

分布式锁在一组进程之间提供了一种互斥机制。在任何时刻，在任何时刻只有一个进程可以持有锁。分布式锁可以在大型分布式系统中实现领导者选举，
在任何时间点，持有锁的那个进程就是系统的领导者。

(1) 为了使用ZooKeeper来实现分布式锁服务，使用顺序 znode 来为那些竞争锁的进程强制排序

思路很简单：

1. 首先指定一个作为锁的 znode，通常用它来描述被锁定的实体，称为 /leader；
2. 然后希望获得锁的客户端创建一些短暂顺序 znode，作为锁 znode 的子节点；
3. 在任何时间点，顺序号最小的客户端将持有锁（有两个客户端差不多同时创建 znode，分别为 /leader/lock-1 和 /leader/lock-2，
那么创建 /leader/lock-1 的客户端将会持有锁，因为它的 znode 顺序号最小。ZooKeeper 服务是顺序的仲裁者，因为它负责分配顺序号）；
4. 通过删除 znode /leader/lock-l 即可简单地将锁释放，另外，如果客户端进程死亡，对应的短暂 znode 也会被删除；
5. 接下来，创建 /leader/lock-2 的客户端将持有锁，因为它顺序号紧跟前一个；
6. 通过创建一个关于 znode 删除的观察，可以使客户端在获得锁时得到通知。

(2) 是申请获取锁的伪代码

1. 在锁 znode下创建一个名为 lock-的短暂顺序 znode，并且记住它的实际路径名(create操作的返回值)；
2. 查询锁 znode 的子节点并且设置一个观察；
3. 如果步骤 l 中所创建的 znode 在步骤 2 中所返回的所有子节点中具有最小的顺序号，则获取到锁。退出；
4. 等待步骤 2 中所设观察的通知并且转到步骤 2。

### 当前问题与方案

#### 羊群效应

(1) 问题

虽然这个算法是正确的，但还是存在一些问题。

第一个问题是这种实现会受到“羊群效应”(herd effect)的影响。考虑有成百上千客户端的情况，所有的客户端都在尝试获得锁，每个客户端都会在
锁 znode 上设置一个观察，用于捕捉子节点的变化。每次锁被释放或另外一个进程开始申请获取锁的时候，观察都会被触发并且每个客户端都会收到
一个通知。  

“羊群效应“就是指大量客户端收到同一事件的通知，但实际上只有很少一部分需要处理这一事件。在这种情况下，只有一个客户端会成功地获取锁，
但是维护过程及向所有客户端发送观察事件会产生峰值流量，这会对 ZooKeeper 服务器造成压力。

(2) 方案解决方案

为了避免出现羊群效应，需要优化通知的条件。

关键在于只有在前一个顺序号的子节点消失时才需要通知下一个客户端，而不是删除（或创建）任何子节点时都需要通知。
如果客户端创建了 znode /leader/lock-1、/leader/lock-2 和 ／leader/lock-3，那么只有当 /leader/lock-2 消失时才需要通知 /leader/lock-3
对照的客户端；/leader/lock-1 消失或有新的 znode /leader/lock-4 加入时，不需要通知该客户端。

#### 可恢复的异常

(1) 问题

这个申请锁的算法目前还存在另一个问题，就是不能处理因连接丢失而导致的 create 操作失败。

在这种情况下，不知道操作是成功还是失败。由于创建一个顺序 znode 是非幂等操作，所以不能简单地重试，因为如果第一次创建已经成功，重试会多出一个永远
删不掉的孤儿 znode(至少到客户端会话结束前）。不幸的结果是将会出现死锁。

(2) 解决方案

问题在于，在重新连接之后客户端不能够判断它是否已经创建过子节点。解决方案是在 znode 的名称中嵌入一个 ID，如果客户端出现连接丢失的情况，重新连接之后
它便可以对锁节点的所有于节点进行检查，看看是否有子节点的名称中包含其 ID。如果有一个子节点的名称包含其 ID，它便知道创建操作已经成功，不需要再创建子节点。
如果没有子节点的名称中包含其 ID，则客户端可以安全地创建一个新的顺序子节点。

客户端会话的ID是一个长整数，并且在 ZooKeeper 服务中是唯一的，因此非常适合在连接丢失后用于识别客户端。可以通过调用 Java ZooKeeper 类的 getSessionld() 方法来获得会话的 ID。

在创建短暂顺序 znode 时应当采用 lock-<sessionld>-这样的命名方式，ZooKeeper 在其尾部添加顺序号之后，znode 的名称会形如 lock-<sessionld>-<sequenceNumber>。
由于顺序号对于父节点来说是唯一的，但对于子节点名并不唯一，因此采用这样的命名方式可以诖子节点在保持创建顺序的同时能够确定自己的创建者。

#### 不可恢复的异常

如果一个客户端的 ZooKeeper 会话过期，那么它所创建的短暂 znode 将会被删除，已持有的锁会被释放，或是放弃了申请锁的位置。使用锁的应用程序应当意识到
它已经不再持有锁，应当清理它的状态，然后通过创建并尝试申请一个新的锁对象来重新启动。注意，这个过程是由应用程序控制的，而不是锁，因为锁是不能预知应用程序需要如何清理自己的状态。

### 场景描述

假设有这样一个场景：两台server ：serverA，serverB 需要在 C 机器上的 /usr/local/a.txt 文件上进行写操作，如果两台机器同时写该文件，那么该文件
的最终结果可能会产生乱序等问题。最先能想到的是 serverA 在写文件前告诉 ServerB “我要开始写文件了，你先别写”，等待收到 ServerB 的确认回复后 ServerA
开始写文件，写完文件后再通知ServerB“我已经写完了”。假设场景中有 100 台机器呢，中间任意一台机器通信中断了又该如何处理？容错和性能问题呢？要能健壮，稳定，
高可用并保持高性能，系统实现的复杂度比较高，从头开发这样的系统代价也很大。幸运的是，有了基于 googlechubby 原理开发的开源的 ZooKeeper 系统。

#### 利用节点名称的唯一性来实现共享锁

ZooKeeper 表面上的节点结构是一个和 unix 文件系统类似的小型的树状的目录结构，ZooKeeper 机制规定：同一个目录下只能有一个唯一的文件名。

算法思路：利用名称唯一性，加锁操作时，只需要所有客户端一起创建 /Leader/lock 节点，只有一个创建成功，成功者获得锁。解锁时，只需删除 /test/Lock 
节点，其余客户端再次进入竞争创建节点，直到所有客户端都获得锁。

#### 利用顺序节点实现共享锁

算法思路：对于加锁操作，可以让所有客户端都去 /lock 目录下创建临时、顺序节点，如果创建的客户端发现自身创建节点序列号是 /lock/ 目录下最小的节点，
则获得锁。否则，监视比自己创建节点的序列号小的节点（当前序列在自己前面一个的节点），进入等待。解锁操作，只需要将自身创建的节点删除即可。

